{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluates our Base Model\n",
    "I have not implemented a test set yet so this uses the validation set\n",
    "and just generates some basic stats and pictures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'STGCN' from 'models.baseSTGCN' (..\\models\\baseSTGCN.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-448450a06748>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodelUtils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloadCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplotPredVsTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdotDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataUtils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloadEnergyData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menergyDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetDatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizeAdjMat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbaseSTGCN\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTGCN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdenormalizedEval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdenormalizeLoad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'STGCN' from 'models.baseSTGCN' (..\\models\\baseSTGCN.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "#visual\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# user functions \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from modelUtils import loadCheckpoint, plotPredVsTrue, dotDict\n",
    "from dataUtils import loadEnergyData, processData, energyDataset, getDatasets, normalizeAdjMat\n",
    "from models.baseSTGCN import STGNN\n",
    "from evaluation.denormalizedEval import denormalizeLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "##### Load our args\n",
    "config_file = \"baseSTGCN_config\"\n",
    "c = importlib.import_module(\"configs.\"+config_file)\n",
    "args = c.args\n",
    "\n",
    "print(args)\n",
    "\n",
    "# update args for evaluation purposes\n",
    "args.save_seq = False\n",
    "args.load_seq = True\n",
    "args.seq_path = os.path.join(\".\" + args.seq_path, \"testingOnly\")\n",
    "\n",
    "\n",
    "processed_dir = \"../data/processed/\"\n",
    "\n",
    "# loss functions to compute\n",
    "mae_criterion = nn.L1Loss()\n",
    "mse_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data\n",
    "if args.load_seq:\n",
    "    # get number of nodes to include\n",
    "    files = os.listdir(args.seq_path)\n",
    "    incl_nodes = max([int(re.search(\"\\d{1,5}\", f).group(0)) for f in files if re.search(\"\\d\", f)])\n",
    "    _, adj_mat = loadEnergyData(processed_dir, incl_nodes = incl_nodes, partial = False)\n",
    "else:\n",
    "    energy_demand, adj_mat = loadEnergyData(processed_dir, incl_nodes = 20, partial = True)\n",
    "\n",
    "_, val_dataset = getDatasets(args, None, validation_only = True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# normalized adjacency matrix with self loop\n",
    "adj_norm = normalizeAdjMat(adj_mat)\n",
    "adj_norm = adj_norm.to(args['device']) \n",
    "\n",
    "val_dataset.inputs.shape, val_dataset.target.shape, adj_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "num_nodes = val_dataset.target.shape[1]\n",
    "num_features = val_dataset.inputs.shape[3]\n",
    "\n",
    "Gnet = STGNN(num_nodes,\n",
    "             num_features,\n",
    "             args.historical_input,\n",
    "             args.forecast_output-1,\n",
    "             args).to(device=args.device)\n",
    "\n",
    "model = loadCheckpoint(Gnet, filename = \"baselineSTGCN.pth\", folder = '../savedModels')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Predictions\n",
    "val_predictions = []\n",
    "val_target = []\n",
    "\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    Gnet.eval()\n",
    "    for vbatch_idx, (vfeatures, vmetadata, vtarget) in enumerate(val_loader):\n",
    "        vfeatures = vfeatures.to(args.device)\n",
    "        vmetadata = vmetadata.to(args.device)\n",
    "        vtarget = vtarget.to(args.device)\n",
    "\n",
    "        # predict \n",
    "        vpreds = Gnet(vfeatures, vmetadata, adj_norm)\n",
    "        mse_loss = mse_criterion(vpreds, vtarget)\n",
    "        mae_loss = mae_criterion(vpreds, vtarget)\n",
    "        rmse_loss = torch.sqrt(mse_criterion(vpreds, vtarget))\n",
    "\n",
    "        # storage and tracking\n",
    "        mse_list.append(mse_loss.detach().cpu().numpy())\n",
    "        mae_list.append(mae_loss.detach().cpu().numpy())\n",
    "        rmse_list.append(rmse_loss.detach().cpu().numpy())\n",
    "        \n",
    "        # store preds and target in correct order\n",
    "        np_vpreds = vpreds.detach().cpu().numpy()\n",
    "        np_vtarget = vtarget.detach().cpu().numpy()\n",
    "        val_predictions.append(np_vpreds)\n",
    "        val_target.append(np_vtarget)\n",
    "        \n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_target = np.concatenate(val_target)\n",
    "\n",
    "mae = np.mean(mae_list)\n",
    "mse = np.mean(mse_list)\n",
    "rmse = np.mean(rmse_list)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPredVsTrue(val_target, val_predictions, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPredVsTrue(val_target, val_predictions, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotPredVsTrue(val_target, val_predictions, 9, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtarget.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_min_max = pd.read_csv(\"../data/processed/Load Min Max Values.csv\")\n",
    "node_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_preds = denormalizeLoad(val_predictions, node_min_max)\n",
    "denorm_target = denormalizeLoad(val_target, node_min_max)\n",
    "\n",
    "mse_criterion(torch.FloatTensor(denorm_preds), torch.FloatTensor(denorm_target)), \\\n",
    "mae_criterion(torch.FloatTensor(denorm_preds), torch.FloatTensor(denorm_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same graphs but using denormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPredVsTrue(denorm_target, denorm_preds, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPredVsTrue(denorm_target, denorm_preds, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./results/TargetValues.pkl\", \"wb\") as f:\n",
    "    pickle.dump(denorm_target, f)\n",
    "\n",
    "with open(\"./results/STGCNPredictions.pkl\", \"wb\") as f:\n",
    "    pickle.dump(denorm_preds, f)\n",
    "    \n",
    "# export normalized \n",
    "with open(\"./results/TargetValues_norm.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_target, f)\n",
    "\n",
    "\n",
    "with open(\"./results/STGCNPredictions_norm.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_prediction, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single node visualized across three time periods\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(denorm_target[2][0], label = \"Node 1: First Time Period\")\n",
    "plt.plot(denorm_target[15][0], label = \"Node 1: Second Time Period\")\n",
    "plt.plot(denorm_target[30][0], label = \"Node 1: Third Time Period\")\n",
    "plt.title(\"Intra-Nodal Variation Across Randomly Selected Time Periods\")\n",
    "plt.xlabel(\"Timestep (hourly)\")\n",
    "plt.ylabel(\"Normalized Load Demand\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three different nodes visualized across the same time period\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(denorm_target[0][1], label = \"Node 1\")\n",
    "plt.plot(denorm_target[0][49], label = \"Node 2\")\n",
    "plt.plot(denorm_target[0][99], label = \"Node 3\")\n",
    "plt.title(\"Inter-Nodal Variation Across Randomly Selected Nodes\")\n",
    "plt.xlabel(\"Timestep (hourly)\")\n",
    "plt.ylabel(\"Normalized Load Demand\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
